{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from helpers import *\n",
    "from lstmHelpers import *\n",
    "\n",
    "import random\n",
    "from random import randrange, shuffle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "\n",
    "filedir = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using classes...\n",
      "Exact Classes: [812, 4977, 9455]\n",
      "Matched Classes: [427, 3667]\n",
      "\n",
      "\n",
      "Time elapsed: 0.013879776000976562 seconds, or 0.00023132960001627604 minutes\n"
     ]
    }
   ],
   "source": [
    "## WRITE STREAM DATA TO FILE\n",
    "start = time.time()\n",
    "total_time = time.time()\n",
    "\n",
    "NUM_CLASSES_OVERRIDE = 5  # If 0, use ALL classes; else, use n classes only\n",
    "num_files_in_stream = 30 * NUM_CLASSES_OVERRIDE\n",
    "order_to_use = []\n",
    "\n",
    "used_classes, unused_classes = load_classes(filedir, NUM_CLASSES_OVERRIDE, display=False)\n",
    "\n",
    "for i in range(NUM_CLASSES_OVERRIDE):\n",
    "    order_to_use.extend([i] * int(num_files_in_stream / NUM_CLASSES_OVERRIDE))\n",
    "    \n",
    "shuffle(order_to_use)\n",
    "generate_data_file(\"trainingInfo.txt\", filedir, used_classes, unused_classes, order_to_use, max_ramp_length=0.25)\n",
    "shuffle(order_to_use)\n",
    "generate_data_file(\"testingInfo.txt\", filedir, used_classes, unused_classes, order_to_use, max_ramp_length=0.25)\n",
    "\n",
    "# code you want to evaluate\n",
    "end = time.time()\n",
    "print(\"Time elapsed: \" + str(end-start) + \" seconds, or \" + str((end-start)/60.0) + \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1%     2%     3%     4%     5%     6%     7%     8%     9%     10%     11%     12%     13%     14%     15%     16%     17%     18%     19%     20%     21%     22%     23%     24%     25%     26%     27%     28%     29%     30%     31%     32%     33%     34%     35%     36%     37%     38%     39%     40%     41%     42%     43%     44%     45%     46%     47%     48%     49%     50%     51%     52%     53%     54%     55%     56%     57%     58%     59%     60%     61%     62%     63%     64%     65%     66%     67%     68%     69%     70%     71%     72%     73%     74%     75%     76%     77%     78%     79%     80%     81%     82%     83%     84%     85%     86%     87%     88%     89%     90%     91%     92%     93%     94%     95%     96%     97%     98%     99%     \n",
      "\n",
      "1%     2%     3%     4%     5%     6%     7%     8%     9%     10%     11%     12%     13%     14%     15%     16%     17%     18%     19%     20%     21%     22%     23%     24%     25%     26%     27%     28%     29%     30%     31%     32%     33%     34%     35%     36%     37%     38%     39%     40%     41%     42%     43%     44%     45%     46%     47%     48%     49%     50%     51%     52%     53%     54%     55%     56%     57%     58%     59%     60%     61%     62%     63%     64%     65%     66%     67%     68%     69%     70%     71%     72%     73%     74%     75%     76%     77%     78%     79%     80%     81%     82%     83%     84%     85%     86%     87%     88%     89%     90%     91%     92%     93%     94%     95%     96%     97%     98%     99%     \n",
      "Time elapsed: 3.84734845161438 seconds, or 0.064122474193573 minutes\n"
     ]
    }
   ],
   "source": [
    "## READ STREAM DATA FROM FILE\n",
    "\n",
    "def generate_composite_stream(audio_matrix, data_dict = None):\n",
    "    cur_percentage = 0\n",
    "    if data_dict == None:\n",
    "        data_dict = {}\n",
    "    \n",
    "    composite_signal_list = []\n",
    "    composite_matches_list = []\n",
    "    \n",
    "    for i in range(len(audio_matrix)):\n",
    "        if (round(i / len(audio_matrix) * 100) != cur_percentage):\n",
    "            cur_percentage = round(i / len(audio_matrix) * 100)\n",
    "            print(str(cur_percentage) + \"%     \", end='')\n",
    "            \n",
    "        filename = audio_matrix[i][2]\n",
    "        if filename in data_dict:\n",
    "            y = data_dict.get(filename)\n",
    "        else:\n",
    "            yt,sr = librosa.load(filename)\n",
    "            y, idx = librosa.effects.trim(yt, top_db=50)\n",
    "            data_dict[filename] = y\n",
    "        warped_y = timewarp(y, audio_matrix[i][6])\n",
    "        noisey_y = apply_noise(warped_y, audio_matrix[i][5])\n",
    "        faded_y = apply_ramp(noisey_y, audio_matrix[i][3], audio_matrix[i][4])\n",
    "        composite_signal_list.append(faded_y)\n",
    "        composite_matches_list.append(np.full(faded_y.shape, audio_matrix[i][1]))\n",
    "    composite_signal = np.array(composite_signal_list)\n",
    "    composite_signal = np.concatenate(composite_signal).ravel()\n",
    "    composite_matches = np.array(composite_matches_list)\n",
    "    composite_matches = np.concatenate(composite_matches).ravel()\n",
    "    return composite_signal, composite_matches, data_dict\n",
    "    \n",
    "start = time.time()\n",
    "\n",
    "audio_matrix = load_stream(name=\"trainingInfo.txt\")\n",
    "composite_signal, composite_matches, preloaded_data = generate_composite_stream(audio_matrix)\n",
    "audio_matrix_test = load_stream(name=\"testingInfo.txt\")\n",
    "composite_signal_test, composite_matches_test, x = generate_composite_stream(audio_matrix_test, data_dict=preloaded_data)\n",
    "\n",
    "composite_matches \n",
    "\n",
    "end = time.time()\n",
    "print(\"\\nTime elapsed: \" + str(end-start) + \" seconds, or \" + str((end-start)/60.0) + \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "1%     2%     3%     4%     5%     6%     7%     8%     9%     10%     11%     12%     13%     14%     15%     16%     17%     18%     "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-aaae7ac10771>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomposite_signal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomposite_matches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTesting Data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mtesting_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomposite_signal_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomposite_matches_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-aaae7ac10771>\u001b[0m in \u001b[0;36mnew_batch\u001b[0;34m(signal, matches)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransposed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mcomp_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesisCode/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesisCode/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    385\u001b[0m                         force_all_finite=\"allow-nan\")\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "n_mels = 128\n",
    "batch_length = 5\n",
    "\n",
    "def new_batch(signal, matches):\n",
    "    signal_batch_length = 2048*3\n",
    "    data = []\n",
    "    classes = []\n",
    "    \n",
    "    batched_frames = []\n",
    "    cur_frame_count = 0\n",
    "    cur_percentage = 0\n",
    "    for frame in signal:\n",
    "        batched_frames.append(frame)\n",
    "        cur_frame_count += 1\n",
    "        if len(batched_frames) == signal_batch_length:\n",
    "            recent_signal = np.asarray(batched_frames)\n",
    "            recent_signal = np.pad(recent_signal, (0, 1024), 'constant', constant_values=(0.0,0.0))\n",
    "            spec = get_spectrogram(recent_signal, 22050, n_mels=n_mels, display=False)\n",
    "            cur_col = 0\n",
    "            transposed = spec.T\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            transposed = scaler.fit_transform(transposed)\n",
    "            comp_cols = []\n",
    "            for i in range(0,batch_length):\n",
    "                comp_cols.append(transposed[cur_col + i])\n",
    "            data.append(comp_cols)\n",
    "            classes.append(int(matches[cur_frame_count-1]))\n",
    "            batched_frames = batched_frames[int(512 / 16):]  # hop length of spectrogram / 16\n",
    "            if (round((cur_frame_count-1) / len(signal) * 100) != cur_percentage):\n",
    "                cur_percentage = round((cur_frame_count-1) / len(signal) * 100)\n",
    "                print(str(cur_percentage) + \"%     \", end='')\n",
    "    \n",
    "    return data, classes\n",
    "\n",
    "def batch(signal, matches):\n",
    "    # Generate batched data list for signal/matches\n",
    "    data = []\n",
    "    classes = []\n",
    "    \n",
    "    spec = get_spectrogram(signal, 22050, n_mels=n_mels, display=False)\n",
    "    cur_col = 0\n",
    "    transposed = spec.T\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    transposed = scaler.fit_transform(transposed)\n",
    "    cur_percentage = 0\n",
    "    for col in transposed:\n",
    "        if cur_col + batch_length >= len(transposed):\n",
    "            break\n",
    "        if (round(cur_col / len(transposed) * 100) != cur_percentage):\n",
    "            cur_percentage = round(cur_col / len(transposed) * 100)\n",
    "            print(str(cur_percentage) + \"%     \", end='')\n",
    "        comp_cols = [col]\n",
    "        for i in range(1,batch_length):\n",
    "            comp_cols.append(transposed[cur_col + i])\n",
    "        data.append(comp_cols)\n",
    "        classes.append(int(matches[int(cur_col / len(transposed) * len(matches))]))\n",
    "        cur_col += 1\n",
    "    return data, classes\n",
    "\n",
    "print(\"Training Data\")\n",
    "training_data, training_classes = new_batch(composite_signal, composite_matches)\n",
    "#print(\"\\nTesting Data\")\n",
    "#testing_data, testing_classes = new_batch(composite_signal_test, composite_matches_test)\n",
    "print()\n",
    "max_spec_length = len(training_data[0])\n",
    "print()\n",
    "training_data = np.array(training_data)\n",
    "#testing_data = np.array(testing_data)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time elapsed: \" + str(end-start) + \" seconds, or \" + str((end-start)/60.0) + \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make/test the LSTM\n",
    "\n",
    "lstm_out = 350\n",
    "batch_size = 64\n",
    "embedding_vector_length = 32\n",
    "dropout = 0.2\n",
    "dropout_r = 0.2\n",
    "\n",
    "number_inputs = NUM_CLASSES_OVERRIDE\n",
    "\n",
    "print(str(number_inputs) + \" \" + str(embedding_vector_length) + \" \" + str(len(training_data[0])) + \" \" + str(dropout) + \n",
    "      \" \" + str(lstm_out) + \" \" + str(dropout_r) + \" \" + str(NUM_CLASSES_OVERRIDE))\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(LSTM(lstm_out, input_shape=(batch_length, n_mels), dropout = dropout, recurrent_dropout = dropout_r))\n",
    "model.add(Dense(NUM_CLASSES_OVERRIDE,activation='softmax'))  #softmax\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])  # sparse_categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(training_data, training_classes, epochs=3, batch_size=batch_size)\n",
    "#scores = model.evaluate(testing_data, testing_classes)\n",
    "#print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "total_end = time.time()\n",
    "print(\"Total Time elapsed: \" + str((total_end-total_time)/60.0) + \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"LSTMModel.h5\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:thesisCode]",
   "language": "python",
   "name": "conda-env-thesisCode-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
