{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from helpers import *\n",
    "\n",
    "import random\n",
    "from random import randrange\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import time\n",
    "\n",
    "\n",
    "filedir = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using classes: \n",
      "[4437, 6090]\n",
      "\n",
      "\n",
      "Time elapsed: 0.012876272201538086 seconds, or 0.00021460453669230143 minutes\n"
     ]
    }
   ],
   "source": [
    "## WRITE STREAM DATA TO FILE\n",
    "\n",
    "num_files_in_stream = 100\n",
    "NUM_CLASSES_OVERRIDE = 2  # If 0, use ALL classes; else, use first n classes only\n",
    "SILENCE_CLASS = -1  # If -1, NO SILENCE CLASS\n",
    "\n",
    "def generate_matched_stream(matches):\n",
    "    if NUM_CLASSES_OVERRIDE > 0:\n",
    "        to_check = random.choice(matches[:NUM_CLASSES_OVERRIDE])\n",
    "    else:\n",
    "        to_check = random.choice(matches)\n",
    "    matched_class = to_check[1]\n",
    "    return to_check\n",
    "\n",
    "def generate_used_stream(used_files, classes_to_use):\n",
    "    if NUM_CLASSES_OVERRIDE > 0:\n",
    "        to_use = randrange(NUM_CLASSES_OVERRIDE)\n",
    "        random_index = classes_to_use[to_use]\n",
    "        matched_class = to_use\n",
    "    else:\n",
    "        to_check = random.choice(matches)\n",
    "        matched_class = random_index\n",
    "    to_check = [used_files[random_index], str(matched_class), used_files[random_index]]\n",
    "    return to_check\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def generate_data_file(name, classes_to_use=None):\n",
    "\n",
    "    # Load used sounds\n",
    "    used_f = open(filedir + '/sampledFiles.txt', \"r\")\n",
    "    used_files = used_f.read().split('\\n')\n",
    "\n",
    "    # Load unused sounds\n",
    "    unused_f = open(filedir + '/unsampledFiles.txt', \"r\")\n",
    "    unused_files = unused_f.read().split('\\n')\n",
    "\n",
    "    # Load matches for unused sound\n",
    "    matches = load_matches(filedir)\n",
    "\n",
    "    silence = random.randint(0, 44100)\n",
    "    \n",
    "    open(filedir + name, 'w').close()  # Delete previous info\n",
    "\n",
    "    with open(filedir + name, \"a\") as training_file:\n",
    "        if SILENCE_CLASS != -1:\n",
    "            training_file.write(str(silence) + \"\\n\")\n",
    "        for i in range(num_files_in_stream):\n",
    "            # Choose a random file, used OR unused  (to change this, alter the if statement to always choose used or unused)\n",
    "            ## UNCOMMENT WHEN FIXED\n",
    "            # len(unused_files) * 1.0 / (len(used_files) + len(unused_files)):\n",
    "            if random.uniform(0.0, 1.0) < 0.0:  \n",
    "                info = generate_matched_stream(matches)\n",
    "            else:\n",
    "                info = generate_used_stream(used_files, classes_to_use)\n",
    "            silence = random.randint(0, 44100)\n",
    "            to_write = info[0] + \" -_- \" + info[1] + \" -_- \" + info[2] + \" \\n\"\n",
    "            if SILENCE_CLASS != -1:\n",
    "                to_write += str(silence) + \"\\n\"\n",
    "            training_file.write(to_write)\n",
    "\n",
    "            \n",
    "if NUM_CLASSES_OVERRIDE > 0:\n",
    "    classes_to_use = []\n",
    "    for i in range(NUM_CLASSES_OVERRIDE):\n",
    "        classes_to_use.append(random.randint(0, len(used_files)))  # THIS WILL NOT WORK IF WE USED MATCHED NON-PERFECT PAIRS\n",
    "print(\"Using classes: \")\n",
    "print(classes_to_use)\n",
    "\n",
    "generate_data_file(\"trainingInfo.txt\", classes_to_use)\n",
    "generate_data_file(\"testingInfo.txt\", classes_to_use)\n",
    "\n",
    "# code you want to evaluate\n",
    "end = time.time()\n",
    "print(\"Time elapsed: \" + str(end-start) + \" seconds, or \" + str((end-start)/60.0) + \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1%     2%     3%     4%     5%     6%     7%     8%     9%     10%     11%     12%     13%     14%     15%     16%     17%     18%     19%     20%     21%     22%     23%     24%     25%     26%     27%     28%     29%     30%     31%     32%     33%     34%     35%     36%     37%     38%     39%     40%     41%     42%     43%     44%     45%     46%     47%     48%     49%     50%     51%     52%     53%     54%     55%     56%     57%     58%     59%     60%     61%     62%     63%     64%     65%     66%     67%     68%     69%     70%     71%     72%     73%     74%     75%     76%     77%     78%     79%     80%     81%     82%     83%     84%     85%     86%     87%     88%     89%     90%     91%     92%     93%     94%     95%     96%     97%     98%     99%     1%     2%     3%     4%     5%     6%     7%     8%     9%     10%     11%     12%     13%     14%     15%     16%     17%     18%     19%     20%     21%     22%     23%     24%     25%     26%     27%     28%     29%     30%     31%     32%     33%     34%     35%     36%     37%     38%     39%     40%     41%     42%     43%     44%     45%     46%     47%     48%     49%     50%     51%     52%     53%     54%     55%     56%     57%     58%     59%     60%     61%     62%     63%     64%     65%     66%     67%     68%     69%     70%     71%     72%     73%     74%     75%     76%     77%     78%     79%     80%     81%     82%     83%     84%     85%     86%     87%     88%     89%     90%     91%     92%     93%     94%     95%     96%     97%     98%     99%     \n",
      "Time elapsed: 3.750398874282837 seconds, or 0.06250664790471396 minutes\n"
     ]
    }
   ],
   "source": [
    "## READ STREAM DATA FROM FILE\n",
    "\n",
    "def load_stream(name, filedir='../'):\n",
    "    training_file = open(filedir + name, \"r\")\n",
    "    info = training_file.read().split('\\n')\n",
    "    data_matrix = []\n",
    "    silence_matrix = []\n",
    "    for inf in info:\n",
    "        segs = inf.split('-_-')\n",
    "        if len(segs) > 1:\n",
    "            try:\n",
    "                data_matrix.append([segs[0].strip(), segs[1].strip(), segs[2].strip()])\n",
    "            except Exception as e:\n",
    "                #print(e)\n",
    "                print(inf)\n",
    "        else:\n",
    "            if segs[0] != '':\n",
    "                silence_matrix.append(segs[0])\n",
    "    training_file.close()\n",
    "    return data_matrix, silence_matrix\n",
    "\n",
    "def generate_composite_stream(audio_matrix, silence_matrix):\n",
    "    cur_percentage = 0\n",
    "    data_dict = {}\n",
    "    if SILENCE_CLASS == -1:\n",
    "        composite_signal = np.zeros(1)\n",
    "        composite_matches = np.full(composite_signal.shape, 0)\n",
    "    else:\n",
    "        composite_signal = np.zeros(int(silence_matrix[0]))\n",
    "        composite_matches = np.full(composite_signal.shape, SILENCE_CLASS)\n",
    "    \n",
    "    for i in range(len(audio_matrix)):\n",
    "        if (round(i / len(audio_matrix) * 100) != cur_percentage):\n",
    "            cur_percentage = round(i / len(audio_matrix) * 100)\n",
    "            print(str(cur_percentage) + \"%     \", end='')\n",
    "            \n",
    "        filename = audio_matrix[i][0]\n",
    "        if filename in data_dict:\n",
    "            y = data_dict.get(filename)\n",
    "        else:\n",
    "            y,sr = librosa.load(filename)\n",
    "            data_dict[filename] = y\n",
    "        composite_signal = np.concatenate((composite_signal, y))\n",
    "        composite_matches = np.concatenate((composite_matches, np.full(y.shape, audio_matrix[i][1])))\n",
    "        if SILENCE_CLASS != -1:\n",
    "            composite_signal = np.concatenate((composite_signal, np.zeros(int(silence_matrix[i+1]))))\n",
    "            composite_matches = np.concatenate((composite_matches, np.full(int(silence_matrix[i+1]), SILENCE_CLASS)))\n",
    "    return composite_signal, composite_matches\n",
    "    \n",
    "start = time.time()\n",
    "\n",
    "audio_matrix, silence_matrix = load_stream(name=\"trainingInfo.txt\")\n",
    "composite_signal, composite_matches = generate_composite_stream(audio_matrix, silence_matrix)\n",
    "audio_matrix_test, silence_matrix_test = load_stream(name=\"testingInfo.txt\")\n",
    "composite_signal_test, composite_matches_test = generate_composite_stream(audio_matrix_test, silence_matrix_test)\n",
    "\n",
    "composite_matches \n",
    "\n",
    "end = time.time()\n",
    "print(\"\\nTime elapsed: \" + str(end-start) + \" seconds, or \" + str((end-start)/60.0) + \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.2860400676727295 seconds, or 0.0047673344612121586 minutes\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "def batch(signal, matches):\n",
    "    # Generate batched data list for signal/matches\n",
    "    \n",
    "    data = []\n",
    "    classes = []\n",
    "    spec = get_all(signal, 22050, \"spectrogram\")[0]\n",
    "    cur_col = 0\n",
    "    for col in spec.T:\n",
    "        data.append(col)\n",
    "        classes.append(int(matches[int(cur_col / len(spec.T) * len(matches))]))\n",
    "        cur_col += 1\n",
    "    return data, classes\n",
    "\n",
    "training_data, training_classes = batch(composite_signal, composite_matches)\n",
    "testing_data, testing_classes = batch(composite_signal_test, composite_matches_test)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time elapsed: \" + str(end-start) + \" seconds, or \" + str((end-start)/60.0) + \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1024 128 0.2 50 0.2 0.2 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_32 (Embedding)     (None, 128, 1024)         2048      \n",
      "_________________________________________________________________\n",
      "lstm_32 (LSTM)               (None, 50)                215000    \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 217,099\n",
      "Trainable params: 217,099\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akarp/anaconda3/envs/thesisCode/lib/python3.6/site-packages/ipykernel/__main__.py:35: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "/home/akarp/anaconda3/envs/thesisCode/lib/python3.6/site-packages/ipykernel/__main__.py:36: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(50, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4259 samples, validate on 4268 samples\n",
      "Epoch 1/3\n",
      "4259/4259 [==============================] - 14s 3ms/step - loss: nan - acc: 0.5450 - val_loss: nan - val_acc: 0.5551\n",
      "Epoch 2/3\n",
      "4259/4259 [==============================] - 11s 3ms/step - loss: nan - acc: 0.5450 - val_loss: nan - val_acc: 0.5551\n",
      "Epoch 3/3\n",
      "4259/4259 [==============================] - 11s 3ms/step - loss: nan - acc: 0.5450 - val_loss: nan - val_acc: 0.5551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f73abb5c9b0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make/test the LSTM\n",
    "\n",
    "lstm_out = 50\n",
    "batch_size = 64\n",
    "embedding_vector_length = 32\n",
    "dropout = 0.2\n",
    "dropout_U = 0.2\n",
    "dropout_W = 0.2\n",
    "\n",
    "used_f = open(filedir + '/sampledFiles.txt', \"r\")\n",
    "used_files = used_f.read().split('\\n')\n",
    "\n",
    "# Load unused sounds\n",
    "unused_f = open(filedir + '/unsampledFiles.txt', \"r\")\n",
    "unused_files = unused_f.read().split('\\n')\n",
    "\n",
    "max_spec_length = len(training_data[0])\n",
    "training_data = pad_sequences(training_data, maxlen=max_spec_length)\n",
    "testing_data = pad_sequences(testing_data, maxlen=max_spec_length)\n",
    "\n",
    "if NUM_CLASSES_OVERRIDE > 0:\n",
    "    num_classes = NUM_CLASSES_OVERRIDE + 1\n",
    "    number_inputs = NUM_CLASSES_OVERRIDE + 1\n",
    "else:\n",
    "    num_classes = len(used_files) + 1\n",
    "    number_inputs = len(used_files) + len(unused_files) + 1\n",
    "if SILENCE_CLASS == -1:\n",
    "    num_classes -= 1\n",
    "    number_inputs -= 1\n",
    "\n",
    "print(str(number_inputs) + \" \" + str(embedding_vector_length) + \" \" + str(max_spec_length) + \" \" + str(dropout) + \n",
    "      \" \" + str(lstm_out) + \" \" + str(dropout_U) + \" \" + str(dropout_W) + \" \" + str(num_classes))\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Embedding(number_inputs, embedding_vector_length, input_length = max_spec_length, dropout = dropout))\n",
    "model.add(LSTM(lstm_out, dropout_U = dropout_U, dropout_W = dropout_W))\n",
    "model.add(Dense(num_classes-1,activation='softmax'))\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(training_data, training_classes, validation_data=(testing_data, testing_classes), epochs=3, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:thesisCode]",
   "language": "python",
   "name": "conda-env-thesisCode-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
